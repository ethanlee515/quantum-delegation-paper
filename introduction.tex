\section{Introduction}
% \XW{ 
% \begin{itemize}
%     \item add a lot of references. 
%     \item comparison with the most relevant results:
%       \begin{itemize}
%           \item Sampling, the only paper; how about classical sampling?
%           \item the following for BQP  
%           \item blind and verifiable ~\cite{GV19}; we  constant round; technique-wise very different. 
%           \item there is a table in~\cite{Grilo19}. Safe to say ~\cite{GV19} only existing blind protocol in the computational setting? 
%           \item all previous either quantum clients, or at least 2 provers. 
%           \item the following for blindess
%           \item Mahadev in her thesis~\cite{mahadev_2018} discussed a bit about the relation between verifiability and blindness. She hoped to get verifiability out of blindness by designing some non-malleable QFHE but failed.   
%           \item what's the high-level message we can say here?  Use QFHE in a different way? It is correct that not much work in the classical setting either.  Maybe existing classical work employs the principle but with different implementation. 
%           \item old approach, first get blindness  (measurement-based, self-testing), and then try to make it verifiable; our approach, first have a verifiable protocol, and upgrade by a QFHE. 
%           \item directly QFHE (blindness) won't give verifiability. some thoughts from Mahadev. 
%           \item 
%       \end{itemize}
% \end{itemize}
% }
Can quantum computation, with potential computational advantages that are intractable for classical computers,
be efficiently verified by classical human being? 
This seemingly paradox has been one of the central problems in quantum complexity theory and delegation of quantum computation~\cite{web:Aaronson}. 
From a philosophical point of view, this question is also known to have a fascinating connection to the \emph{falsifiability} of quantum mechanics in the potential high complexity regime~\cite{survey:AV12}. 

A complexity theoretic formulation of this problem by Gottesman in 2004~\cite{web:Aaronson} asks the possibility for an efficient classical verifier/client (a $\BPP$ machine) to verify the output of an
efficient quantum prover (a $\BQP$ machine).
In the absence of techniques for directly tackling this question, earlier feasibility results on this problem have been focusing on two weaker formulations. 
The first type of feasibility results (e.g.,~\cite{BFK09,arXiv:ABOEM17,PR:FitKas17,mf16}) considers the case where the $\BPP$ verifier is equipped with limited quantum power.
The second type of feasibility results (e.g,~\cite{Nat:RUV13, CGJV19, Gheorghiu_2015, HPF15})
considers a $\BPP$ verifier interacting with at least two entangled, non communicating quantum provers. 
In a recent breakthrough, Mahadev~\cite{FOCS:Mahadev18a} proposed the first protocol of classical verification of quantum computation (CVQC) whose soundness is based on a widely recognized computational assumption that the learning with error (LWE) problem~\cite{JACM:Regev09} is hard for $\BQP$ machines. 
The technique invented therein has inspired many  subsequent developments of CVQC protocols with improved parameters and functionality (e.g.~\cite{FOCS:GheVid19,arXiv:AlaChiHun19,arXiv:ChiaChungYam19}). 
We refer curious readers to the survey~\cite{survey:GKK19} for details. 

With the newly developed techniques, we revisit the classical verification of quantum computation problem from both a philosophical and a practical point of view. 
We observe that the \emph{sampling} version of $\BQP$ (e.g., the $\SampBQP$ formulated by Aaronson~\cite{aaronson_2013}) might be a more appropriate notion to serve the purpose of the original problem. 
Philosophically, the outcomes of quantum mechanical experiments are usually samples or statistical information, which is well demonstrated in the famous double-slit experiment. 
Moreover, a lot of quantum algorithms from Shor's~\cite{Shor} and Grover's~\cite{Grover} algorithms to some recent developments in machine learning and optimization (e.g.~\cite{brando_et_al:LIPIcs:2019:10603, AGGW17,pmlr-v97-li19b}) contain a significant quantum sampling component. 
The fact that almost all quantum supremacy tasks (e.g.,~\cite{Boson, IQP, nature-google}) are sampling ones strengthens the importance of $\SampBQP$. 
Even though the relation between $\BQP$ and $\SampBQP$ is relatively understood in the plain model, what are the potential technical challenges raised by their subtle differences in the context of CVQC and how to resolve them is however far from clear. 
In particular, it is a prior unclear whether it is feasible to develop a CVQC protocol for $\SampBQP$ based on Mahadev's technique~\cite{FOCS:Mahadev18a}.

Another desirable property of CVQC protocols is the \emph{blindness} where the prover cannot distinguish the particular computation in the protocol from another one of the same size, and hence is blind about the client's input. 
Historically, blindness has been achieved in the weaker formulations of CVQC based on various techniques: e.g., the measurement-based quantum computation exploited in~\cite{BFK09}, the quantum authentication scheme exploited in~\cite{arXiv:ABOEM17}, and the self-testing technique exploited in~\cite{Nat:RUV13}. 
Moreover, the blindness property is known to be helpful to establish the verifiability of CVQC protocols. However, this is never an easy task. 
See for example the significant amount of efforts to add verifiability to blind CVQC protocols in~\cite{FK17}. 
Achieving both blindness and verifiability on top of Mahadev's technique~\cite{FOCS:Mahadev18a} is a conceivably much more challenging task. 
The only successful attempt~\cite{FOCS:GheVid19} so far applies Mahadev's technique to the measurement-based quantum computation, 
whereas the analysis is still very specific to the construction. 
Could there be a \emph{generic} way to achieve blindness and verifiability for CVQC protocols at the same time?


\vspace{2mm} \noindent \textbf{Contribution.} We provide \emph{affirmative} solutions to both of our questions. 
In particular, we demonstrate the feasibility of the classical verification of quantum sampling by 
constructing a constant-round CVQC protocol for $\SampBQP$. 
Our protocol leverages the Hamiltonian model and the computational X-Z measurement from~\cite{FOCS:Mahadev18a}.
However, a significant amount of new techniques have been developed to deal with the difference between $\SampBQP$ and $\BQP$, which will be highlighted in the technical contribution below. Precisely, 

\XW{Insert the theorem statement for the first result here!}

Blindness Part 
\begin{itemize}
    \item Motivating Blindness
    \item To ADD
    \item Combine both for the results. 
\end{itemize}

Related work
\begin{itemize}
    \item Comparison with related works here?
\end{itemize}

%       \begin{itemize}
%           \item Sampling, the only paper; how about classical sampling?
%           \item the following for BQP  
%           \item blind and verifiable ~\cite{GV19}; we  constant round; technique-wise very different. 
%           \item there is a table in~\cite{Grilo19}. Safe to say ~\cite{GV19} only existing blind protocol in the computational setting? 
%           \item all previous either quantum clients, or at least 2 provers. 
%           \item the following for blindess
%           \item Mahadev in her thesis~\cite{mahadev_2018} discussed a bit about the relation between verifiability and blindness. She hoped to get verifiability out of blindness by designing some non-malleable QFHE but failed.   
%           \item what's the high-level message we can say here?  Use QFHE in a different way? It is correct that not much work in the classical setting either.  Maybe existing classical work employs the principle but with different implementation. 
%           \item old approach, first get blindness  (measurement-based, self-testing), and then try to make it verifiable; our approach, first have a verifiable protocol, and upgrade by a QFHE. 
%           \item directly QFHE (blindness) won't give verifiability. some thoughts from Mahadev. 
%
%           technical comparison with the past parallel % reptiton. 
%           \item 


\vspace{2mm} \noindent \textbf{Techniques.}


Technical Contribution
\begin{itemize}
    \item Technical contribution. 
    \item 2nd de finetti have independence; but latter, the analysis, computational parallel repetition. 
    \item TO ADD for both parts. 
\end{itemize}

\vspace{2mm} \noindent \textbf{Open Questions.}
\begin{itemize}
    \item Open questions, and also explain for some associated high-cost. specifically 
    \item T dependence. In general, improve the efficiency.  
    \item negligible soundness error.  compare with classical? what's the state-of-the art. 
\end{itemize}


% \Ethan{This section is currently all rough draft. We'll probably rewrite almost all of it.}

% \Ethan{application: verifiable private constant round delegation}

% \Ethan{Below is my attempt to talk about it in my SoP}

% It was proven that BQP=BQIP\hannote{who  when and cite}. That is, if a quantum computer can efficiently solve a given decision problem, then it can also efficiently convince a classical machine of its solution. I'm generalizing this to arbitrary efficient quantum computations. The proof for decision problems involves the classical verifier reducing the problem to a local Hamiltonian instance; the quantum prover would then commit its certificate and act as the verifier’s trusted measurement device as put forth in ``Classical Verification of Quantum Computations" by Mahadev. It isn't as trivial as it may seem. Repeating the scheme for each qubit loses the information carried by entanglements and throws off the joint distribution between qubits. Simply measuring the entire output register instead is difficult to analyze. For decision problems, it’s not hard to argue that a malicious prover cannot do better than sending identical copies of some pure state unentangled with each others. That same reasoning doesn't apply here a priori. I've been trying to get a grasp on the particular structure of the local Hamiltonian reduction in order to better analyze it.

% \Ethan{Below is my attempt to talk about it in my research proposal}

% We are interested in delegating quantum computations from a classical client to an untrusted quantum server. Under this setting, the client would send the server a quantum circuit and an initial state. Then, through interaction with the honest server, the client obtains a measurement result as if he measured the true output of the circuit. If the server attempts to deceive the client, the client should reject it. The case where the circuit encodes a decision problem has been well-studied, and we're now trying to generalize those results to circuits with possibly many bits of output.

% If the circuit encodes a decision problem, then by considering adiabatic quantum computation, there exists a reduction to local Hamiltonian. Local Hamiltonian is QMA-complete, so there's a certificate for every yes-instance, and no valid certificates for any no-instances. An introduction can be found in Kitaev, Shen, and Vyalyi's "Classical and Quantum Computation". Furthermore, Biamonte and Love's "Realizable Hamiltonians for Universal Adiabatic Quantum Computers" states that these local Hamiltonians have very simple forms, which in turn implies that in order to check such certificates one only requires abilities to receive qubits and perform X/Z measurements. Based on this observation, Mahadev constructed a protocol in "Classical Verification of Quantum Computations" which, under the LWE assumption (a widely believed conjecture in quantum cryptography), allows the prover to commit qubits and act as the verfier's trusted X/Z measurement device. This solves the delegation of decision problem from a fully classical client to a quantum server.

% To generalize delegation of quantum computations to allow long output, simply repeating the known protocol for every output qubit doesn't work. The joint probability distribution between qubits would be incorrect due to entanglements. In fact, generally the output qubits encode sampling problems rather than decision problems. Furthermore, for decision problems one can argue that the prover's optimal strategy is to send identical copies of a certificate, so Chernoff bound can be applied, but said argument doesn't generalize to sampling problems either. To overcome these challenges, we start by modifying the local Hamiltonian construction so it is compatible with long output. We then analyze our protocol's soundness more carefully, before using Mahadev's result as a black box to solve the long output case too for fully classical clients.

% A possible application for our long output protocol is to make the computation not only verifiable, but also private in the sense of homomorphic encryptions. That is, the input is encrypted before being sent to the server. The server computes on the encrypted input, obtaining an encrypted output. The client then receives and decrypts the output. Here we can combine results from Mahadev's "Classical Homomorphic Encryption for Quantum Circuits" with our long output scheme. The client can simply send the homomorphic evaluation circuit to the server with the encrypted input.
