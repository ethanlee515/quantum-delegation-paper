\section{Introduction}
% \XW{ 
% \begin{itemize}
%     \item add a lot of references. 
%     \item comparison with the most relevant results:
%       \begin{itemize}
%           \item Sampling, the only paper; how about classical sampling?
%           \item the following for BQP  
%           \item blind and verifiable ~\cite{GV19}; we  constant round; technique-wise very different. 
%           \item there is a table in~\cite{Grilo19}. Safe to say ~\cite{GV19} only existing blind protocol in the computational setting? 
%           \item all previous either quantum clients, or at least 2 provers. 
%           \item the following for blindess
%           \item Mahadev in her thesis~\cite{mahadev_2018} discussed a bit about the relation between verifiability and blindness. She hoped to get verifiability out of blindness by designing some non-malleable QFHE but failed.   
%           \item what's the high-level message we can say here?  Use QFHE in a different way? It is correct that not much work in the classical setting either.  Maybe existing classical work employs the principle but with different implementation. 
%           \item old approach, first get blindness  (measurement-based, self-testing), and then try to make it verifiable; our approach, first have a verifiable protocol, and upgrade by a QFHE. 
%           \item directly QFHE (blindness) won't give verifiability. some thoughts from Mahadev. 
%           \item 
%       \end{itemize}
% \end{itemize}
% }
Can quantum computation, with potential computational advantages that are intractable for classical computers,
be efficiently verified by classical human being? 
This seemingly paradox has been one of the central problems in quantum complexity theory and delegation of quantum computation~\cite{web:Aaronson}. 
From a philosophical point of view, this question is also known to have a fascinating connection to the \emph{falsifiability} of quantum mechanics in the potential high complexity regime~\cite{survey:AV12}. 

A complexity theoretic formulation of this problem by Gottesman in 2004~\cite{web:Aaronson} asks the possibility for an efficient classical verifier (a $\BPP$ machine) to verify the output of an
efficient quantum prover (a $\BQP$ machine).
In the absence of techniques for directly tackling this question, earlier feasibility results on this problem have been focusing on two weaker formulations. 
The first type of feasibility results (e.g.,~\cite{BFK09,arXiv:ABOEM17,PR:FitKas17,mf16}) considers the case where the $\BPP$ verifier is equipped with limited quantum power.
The second type of feasibility results (e.g,~\cite{Nat:RUV13, CGJV19, Gheorghiu_2015, HPF15})
considers a $\BPP$ verifier interacting with at least two entangled, non communicating quantum provers. 
In a recent breakthrough, Mahadev~\cite{mahadev_2018} proposed the first protocol of classical verification of quantum computation (CVQC) whose soundness is based on a widely recognized computational assumption that the learning with error (LWE) problem~\cite{JACM:Regev09} is hard for $\BQP$ machines. 
The technique invented therein has inspired many  subsequent developments of CVQC protocols with improved parameters and functionality (e.g.~\cite{FOCS:GheVid19,arXiv:AlaChiHun19,arXiv:ChiaChungYam19}). 
We refer curious readers to the survey~\cite{survey:GKK19} for details. 

With the newly developed techniques, we revisit the verification of quantum computation problem from both a philosophical and a practical point of view. 
Our first observation is that the sampling version of $\BQP$ (e.g., as formulated in Aaronson~\cite



\XW{motivation: say with the development, a first question is to revisit the BQP formulation; emphasize that SampBQP is a more important question; and poses some technique difficulty. }


\XW{Another feature one hope is the blindness}


\vspace{1mm} \noindent \textbf{Contribution.}

\XW{list the main results; + comparison with all related works}


Sampling Part
\begin{itemize}
    \item Motivating Sampling.  and a little bit why sampling could be different and hard. (Aaronson's paper?) 
    \item To ADD
    \item 2nd de finetti have independence; but latter, the analysis, computational parallel repetition. 
    \item emphasize the feasibility. 
\end{itemize}

Blindness Part 
\begin{itemize}
    \item Motivating Blindness
    \item To ADD
    \item Combine both for the results. 
\end{itemize}

Related work
\begin{itemize}
    \item Comparison with related works here?
\end{itemize}


\XW{then the technical contribution}

Technical Contribution
\begin{itemize}
    \item Technical contribution. 
    \item TO ADD for both parts. 
\end{itemize}

Open Questions
\begin{itemize}
    \item Open questions, and also explain for some associated high-cost. specifically 
    \item T dependence. In general, improve the efficiency.  
    \item negligible soundness error.  compare with classical? what's the state-of-the art. 
\end{itemize}

Organization


% \Ethan{This section is currently all rough draft. We'll probably rewrite almost all of it.}

% \Ethan{application: verifiable private constant round delegation}

% \Ethan{Below is my attempt to talk about it in my SoP}

% It was proven that BQP=BQIP\hannote{who  when and cite}. That is, if a quantum computer can efficiently solve a given decision problem, then it can also efficiently convince a classical machine of its solution. I'm generalizing this to arbitrary efficient quantum computations. The proof for decision problems involves the classical verifier reducing the problem to a local Hamiltonian instance; the quantum prover would then commit its certificate and act as the verifier’s trusted measurement device as put forth in ``Classical Verification of Quantum Computations" by Mahadev. It isn't as trivial as it may seem. Repeating the scheme for each qubit loses the information carried by entanglements and throws off the joint distribution between qubits. Simply measuring the entire output register instead is difficult to analyze. For decision problems, it’s not hard to argue that a malicious prover cannot do better than sending identical copies of some pure state unentangled with each others. That same reasoning doesn't apply here a priori. I've been trying to get a grasp on the particular structure of the local Hamiltonian reduction in order to better analyze it.

% \Ethan{Below is my attempt to talk about it in my research proposal}

% We are interested in delegating quantum computations from a classical client to an untrusted quantum server. Under this setting, the client would send the server a quantum circuit and an initial state. Then, through interaction with the honest server, the client obtains a measurement result as if he measured the true output of the circuit. If the server attempts to deceive the client, the client should reject it. The case where the circuit encodes a decision problem has been well-studied, and we're now trying to generalize those results to circuits with possibly many bits of output.

% If the circuit encodes a decision problem, then by considering adiabatic quantum computation, there exists a reduction to local Hamiltonian. Local Hamiltonian is QMA-complete, so there's a certificate for every yes-instance, and no valid certificates for any no-instances. An introduction can be found in Kitaev, Shen, and Vyalyi's "Classical and Quantum Computation". Furthermore, Biamonte and Love's "Realizable Hamiltonians for Universal Adiabatic Quantum Computers" states that these local Hamiltonians have very simple forms, which in turn implies that in order to check such certificates one only requires abilities to receive qubits and perform X/Z measurements. Based on this observation, Mahadev constructed a protocol in "Classical Verification of Quantum Computations" which, under the LWE assumption (a widely believed conjecture in quantum cryptography), allows the prover to commit qubits and act as the verfier's trusted X/Z measurement device. This solves the delegation of decision problem from a fully classical client to a quantum server.

% To generalize delegation of quantum computations to allow long output, simply repeating the known protocol for every output qubit doesn't work. The joint probability distribution between qubits would be incorrect due to entanglements. In fact, generally the output qubits encode sampling problems rather than decision problems. Furthermore, for decision problems one can argue that the prover's optimal strategy is to send identical copies of a certificate, so Chernoff bound can be applied, but said argument doesn't generalize to sampling problems either. To overcome these challenges, we start by modifying the local Hamiltonian construction so it is compatible with long output. We then analyze our protocol's soundness more carefully, before using Mahadev's result as a black box to solve the long output case too for fully classical clients.

% A possible application for our long output protocol is to make the computation not only verifiable, but also private in the sense of homomorphic encryptions. That is, the input is encrypted before being sent to the server. The server computes on the encrypted input, obtaining an encrypted output. The client then receives and decrypts the output. Here we can combine results from Mahadev's "Classical Homomorphic Encryption for Quantum Circuits" with our long output scheme. The client can simply send the homomorphic evaluation circuit to the server with the encrypted input.
