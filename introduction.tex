\section{Introduction}
Can quantum computation, with potential computational advantages that are intractable for classical computers,
be efficiently verified by classical means?
This seeming paradox has been one of the central problems in quantum complexity theory and delegation of quantum computation~\cite{web:Aaronson}.
From a philosophical point of view, this question is also known to have a fascinating connection to the \emph{falsifiability} of quantum mechanics in the high complexity regime~\cite{survey:AV12}.

A complexity theoretic formulation of this problem by Gottesman in 2004~\cite{web:Aaronson} asks about the possibility for an efficient classical verifier (a $\BPP$ machine) to verify the output of an
efficient quantum prover (a $\BQP$ machine).
In the absence of techniques for directly tackling this question, earlier feasibility results on this problem have been focusing on two weaker formulations.
The first type of feasibility results (e.g.,~\cite{BFK09,arXiv:ABOEM17,FK17,mf16}) considers the case where the $\BPP$ verifier is equipped with limited quantum power.
The second type of feasibility results (e.g,~\cite{Nat:RUV13, CGJV19, Gheorghiu_2015, HPF15})
considers a $\BPP$ verifier interacting with at least two entangled, non-communicating quantum provers.
In a recent breakthrough, Mahadev~\cite{FOCS:Mahadev18a} proposed the first protocol of Classical Verification of Quantum Computation (CVQC) whose soundness is based on a widely recognized computational assumption that the learning with error (LWE) problem~\cite{JACM:Regev09} is hard for $\BQP$ machines.
The technique invented therein has inspired many  subsequent developments of CVQC protocols with improved parameters and functionality (e.g.,~\cite{FOCS:GheVid19,arXiv:AlaChiHun19,arXiv:ChiaChungYam19}).
On the other side, there are known complexity-theoretic limitations on the feasibility of blind CVQC in the information-theoretical setting (e.g.~\cite{aaronson_et_al:LIPIcs:2019:10582}).

With the newly developed techniques, we revisit the classical verification of quantum computation problems from both a philosophical and a practical point of view.
We first observe that the \emph{sampling} version of $\BQP$ (e.g., the class $\SampBQP$ formulated by Aaronson~\cite{aaronson_2013}) might be a more appropriate notion to serve the purpose of the original problem.
Philosophically, the outcomes of quantum mechanical experiments are usually samples or statistical information, which is well demonstrated in the famous double-slit experiment.
Moreover, a lot of quantum algorithms from Shor's~\cite{Shor} and Grover's~\cite{Grover} algorithms to some recent developments in machine learning and optimization (e.g.~\cite{brando_et_al:LIPIcs:2019:10603, AGGW17,pmlr-v97-li19b}) contain a significant quantum sampling component.
The fact that almost all quantum supremacy tasks (e.g.,~\cite{Boson, IQP, nature-google}) are sampling ones also suggests the relevance of delegation for quantum sampling problems.

It is worthwhile noting that there is a simple reduction of the delegation of \emph{classical} sampling problems to decision ones: for example, the verifier can fix a seed for pseudo-randomness, send it to the prover, and then the sampling outcome can be computed  bit-by-bit under this pseudo-random seed in a deterministic way. As such, classical literature of delegation of computation primarily focuses on delegation of decision problems. 

Unfortunately, it is unclear about how to make this de-randomization trick work for the delegation of $\SampBQP$, 
as randomness in quantum computation inherently comes from quantum mechanics rather than classical probability theory. 
Thus, it seems that the delegation of $\SampBQP$ needs to take a different technical route.
One natural starting point is Mahadev's CVQC protocol~\cite{FOCS:Mahadev18a} for $\BQP$. 
Indeed, some nice features of Mahadev's protocol (e.g., allowing X-Z measurements on any qubit) suggest the feasibility of generating  multi-bit measurement outcomes with the protocol. 
However, as highlighted below in the technical introduction section, there are also important drawbacks of Mahadev's protocol in its current form, which makes it hard to allow a CVQC protocol for $\SampBQP$ with desired performance. As we shall see, such difficulties do not hold for the decision problem, which demonstrates an important difference between CVQC protocols for the decision and the sampling problems. 

Another desirable property of CVQC protocols is the \emph{blindness} where the prover cannot distinguish the particular computation in the protocol from another one of the same size, and hence is blind about the client's input.
Historically, blindness has been achieved in the weaker formulations of CVQC based on various techniques: e.g., the measurement-based quantum computation exploited in~\cite{BFK09}, the quantum authentication scheme exploited in~\cite{arXiv:ABOEM17}, and the self-testing technique exploited in~\cite{Nat:RUV13}.
Moreover, the blindness property is known to be helpful to establish the verifiability of CVQC protocols. However, this is never an easy task.
See for example the significant amount of efforts to add verifiability to blind computation protocol in~\cite{FK17}.
It is also known that another important primitive called the Quantum Fully Homomorphic Encryption (QFHE)~\cite{BJ15, DSS16, LC18, NS18, OTF18, mahadev_qfhe} should be helpful for establishing blindness: this is intuitive since QFHE allows fully homomorphic operations on encrypted quantum data. 
Indeed, in another paper~\cite{mahadev_qfhe}, Mahadev constructed the first leveled QFHE based on similar techniques and computational assumptions from~\cite{FOCS:Mahadev18a}.
The constructed QFHE automatically implies a blind CVQC protocol, however, without verifiability.
Extending this protocol with verifiability seems challenging as hinted by failed attempts in Section 2.2.2 of Mahadev's PhD thesis~\cite{mahadev_2018}.

In fact, most existing blind and/or verifiable protocols for delegation of quantum computation require a notable amount of effort in achieving each property respectively. 
The only successful CVQC protocol~\cite{FOCS:GheVid19} of achieving both so far applies Mahadev's technique~\cite{FOCS:Mahadev18a} to the measurement-based quantum computation, whereas the analysis is still very specific to the construction.
Could there be a \emph{generic} way to achieve blindness and verifiability for CVQC protocols at the same time, say by leveraging the state-of-the-art CVQC~\cite{FOCS:Mahadev18a} and QFHE~\cite{mahadev_qfhe} protocols? 


\vspace{2mm} \noindent \textbf{Contribution.} We provide \emph{affirmative} solutions to both of our questions.
In particular, we demonstrate the feasibility of the classical verification of quantum sampling by
constructing a constant-round CVQC protocol for $\SampBQP$, based on the quantum LWE (QLWE) assumption that the learning-with-error problem is hard for BQP machines. 
$\SampBQP$ is the sampling version of $\BQP$ formulated by Aaronson~\cite{aaronson_2013}.
Formally, $\SampBQP$ consists of sampling problems $(D_x)_{x\in\zo^*}$ that can be approximately sampled by a $\BQP$ machine with an inverse polynomial accuracy. See Section~\ref{sec:samp_definition} for further discussions on $\SampBQP$ and the precise definition of CVQC for $\SampBQP$.   \begin{theorem}[informal] \label{thm:qpip0-informal}
Assuming the QLWE assumption, there exists a four-message CVQC protocol for all sampling problems in $\SampBQP$ with negligible completeness error and computational soundness.
\end{theorem}

Our second contribution is a simple yet powerful \emph{generic}  compiler that transforms any CVQC protocol to a blind one while preserving verifiability, building on top of QFHE. 
Precisely, we leverage QFHE (especially the one from~\cite{mahadev_qfhe}) to transform any CVQC protocol to \emph{a blind one with the same number of round communication, while preserving completeness and soundness properties}.
As a result, one can \emph{upgrade} every verifiable CVQC protocol with blindness almost for free with the help of QFHE.
Conceptually, we take a very different approach from previous ones  (e.g.,~\cite{FK17} as well as failed attempts in~\cite{mahadev_2018}) which use the blindness as the start point and then work to extend it with verifiability.
Instead, our strategy is to simulate a (verifiable) CVQC protocol under QFHE per each message.
To that end, we do require a special property of QFHE that the classical part of the ciphertext can be operated on separately from the quantum part, which is satisfied by the construction from~\cite{mahadev_qfhe}.
Our construction makes a modular use of QFHE and only requires a minor technicality in the analysis, which will be explained below. 
%As a result, we obtain
\begin{theorem}[informal]
Assuming the QLWE assumption, there exists a protocol compiler that transforms any CVQC protocol $\Pi$ to a CVQC protocol $\Piblind$ that achieves blindness while preserves its round complexity, completeness, and soundness.
\end{theorem}

As a simple corollary of combining both results above, we achieve a constant-round blind CVQC protocol for $\SampBQP$. 
\begin{theorem}[informal]
        Assuming the QLWE assumption, there exists a blind, four-message CVQC protocol for all sampling problems in $\SampBQP$ with negligible completeness error and computational soundness.
\end{theorem}

We also construct the first blind and constant-round CVQC protocol for $\BQP$ by applying our compiler to the parallel repetition of Mahadev's protocol for $\BQP$ from \cite{arXiv:ChiaChungYam19, arXiv:AlaChiHun19}.

\begin{theorem}[informal]
    Assuming the QLWE assumption, there exists a blind, four-message CVQC protocol for all languages in $\BQP$ with negligible completeness and soundness errors.
\end{theorem}

To the authors' best knowledge, we are the first to study CVQC protocols for $\SampBQP$ and establish a generic compiler to upgrade CVQC protocols with blindness.
Our result also entails a \emph{constant-round} blind and verifiable CVQC protocol for $\BQP$.
The closest result to ours is by Gheorghiu and Vidick~\cite{FOCS:GheVid19} which shows such a CVQC protocol for $\BQP$, however, with a polynomial number of rounds.
Their protocol was obtained by first constructing a remote state preparation primitive and then combining it with an existing blind and verifiable protocol~\cite{FK17} where the verifier has some limited quantum power.
Our technical approach is quite different and seems incomparable.

\vspace{2mm} \noindent \textbf{Techniques.} Let us revisit Mahadev's CVQC protocol~\cite{FOCS:Mahadev18a} first for some technical background. 
Following~\cite{FOCS:Mahadev18a}, we formally define $\QPIP_{\tau}$ as classes of CVQC protocols where $\tau$ refers to the size of (local) quantum memory in the possession of the classical verifier, or equivalently, the limited quantum power of the verifier\footnote{Intuitively, the size of local quantum memory limits the size of qubits that a general (entangled) quantum operation can be operated on at the same time. However, the verifier is still able to operate on all the qubits in a streaming fashion: i.e., receive the qubits sent from the pover and discard some local qubits in the case of overflow.}. 
A precise definition is given in Section~\ref{sec:qpip_def}. 
It is known that $\BQP$ can be efficiently verified by a classical verifier that can perform a single qubit $X$ or $Z$ measurement~\cite{PhysRevA.93.022326, mf16}, by reducing any $\BQP$ problem to a local Hamiltonian problem where each term consists of  $X$ and $Z$ only. This leads to a $\QPIP_1$ protocol for $\BQP$.
The main contribution of Mahadev~\cite{FOCS:Mahadev18a} can hence be deemed as a way to compile this $\QPIP_1$ protocol into a $\QPIP_0$ protocol (i.e., with a fully classical verifier).

Precisely, to leverage Mahadev's construction, one needs to start with a $\QPIP_1$ protocol with very small completeness and soundness errors, which will then be compiled into a $\QPIP_0$ protocol with a small completeness error, but a close-to-$1/2$ soundness error under the QLWE assumption\footnote{Mahadev only proved soundness error close-to-$3/4$ for her protocol, but it was later proved~\cite{arXiv:ChiaChungYam19, arXiv:AlaChiHun19} that her protocol actually achieves close-to-$1/2$ soundness error.}.  
This large soundness error is due to the current structure of Mahadev's protocol that consists of the \emph{testing} round and the \emph{Hadamard} round, each of which happens with a half chance. 
At a high level, the protocol verifies the behavior of the prover in the testing round, while assumes that the prover behaves honestly and all the X-Z measurements are correct in the Hadamard round. 
The soundness is obtained by observing that the dishonest prover cannot cheat in both rounds, while cheating in one round alone is possible which leads to a large soundness error. 
This less desirable soundness error, as well as other parameters, has been subsequently improved in \cite{arXiv:AlaChiHun19, arXiv:ChiaChungYam19} by a parallel repetition of Mahadev's original CVQC protocol in the computational setting. 

The first and important step in the case of $\SampBQP$ is to identify an appropriate definition of corresponding CVQC protocols. In contrast to the single bit $\mathrm{Accept/Reject}$ information for the decision $\BQP$, any $\SampBQP$ problem needs to always output a sample of the desired distribution.
In the context of CVQC protocols, it means that the protocol should allow the verifier to generate a desired output sample whenever the protocol accepts. 
(Of course, when the prover cheats, the verifier will reject and no further output is required in that case.)
Intuitively, the soundness requires that the verifier $V$ should never be ``cheated'' to accept and output an incorrect sample even when interact with a malicious prover. We formalize this by a strong \emph{simulation-based} definition (Section~\ref{sec:samp_definition}), where we require that the joint distribution of the decision bit $d \in \set{\Acc, \Rej}$ and the output $z$ (which is $\bot$ when $d = \Rej$) is $\eps$-close (in the either statistical or computational sense) to an ``ideal distribution'' $(d,z_{ideal})$, where $z_{ideal}$ is sampled from $D_x$ when $d = \Acc$ and set to $\bot$ when $d = \Rej$.

To the best of our knowledge, we are the first to formally define delegation of quantum sampling problems. We also note that while several constructions in the relaxed models (e.g., verifiable blind computation~\cite{FK17}) can be naturally generalized to delegate quantum sampling problem and allow the verifier to learn multi-bit outputs, it seems non-trivial to show that these constructions achieve the simulation-based soundness property we defined. Proving soundness of these constructions for delegating $\SampBQP$ is an interesting open question.

The requirement that the verifier needs to output a sample when accepts exposes one drawback of Mahadev's protocol: 
while it is possible, as we will show, to generate desired output samples in the Hadamard round, 
it is however unclear about how to generate such a sample when the protocol accepts in the testing round, since the sampling information stored in the quantum state will be disturbed by the testing step. 

As a result, in order to construct a $\QPIP_0$ protocol for $\SampBQP$,  one needs to (1) construct a $\QPIP_1$ protocol for $\SampBQP$ with very small completeness and soundness errors, and (2) amend Mahadev's construction so that the protocol will generate the desired sample whenever it accepts. 
Our contribution is a solution to address the above two technical challenges. 

\vspace{2mm} \noindent \textbf{Construction of a $\QPIP_0$ protocol for $\SampBQP$}. 
Following the aforementioned outline, our construction can be divided into three key technical steps. 

\vspace{2mm} \noindent \emph{$\diamond$ Reducing  $\SampBQP$ to the local Hamiltonian problem}: We will continue to employ the local Hamiltonian technique~\cite{kitaev2002classical} and its ground state (known as the history state) as a key technical ingredient to certify the $\SampBQP$ circuits. 
Recall that the $\QPIP_1$ protocol for $\BQP$ in Mahadev's construction also comes from a reduction of $\BQP$ to a $X$-$Z$-only local Hamiltonian problem. 

However, there are important differences between the cases for $\BQP$ and $\SampBQP$. Recall that the original construction of local Hamiltonian $H$ for $\BQP$ (or $\QMA$) contains two parts $H=H_{\mathrm{circuit}}+ H_{\mathrm{out}}$.
Roughly speaking, $H_{\mathrm{circuit}}$ helps guarantee its ground space only contains \emph{valid} history states with correct input and circuit evolution, while $H_{\mathrm{out}}$'s energy encodes the 0/1 output for $\BQP$ circuits.
Thus, the outcome of a $\BQP$ instance can be encoded by the \emph{ground energy} of $H$.

In the case of $\SampBQP$, one needs to output both Accept/Reject information as well as a sample from the desired distribution if the protocol accepts. 
To that end, one hopes to certify the validity of the entire history state which is the ground state of $H_{\mathrm{circuit}}$, rather than the ground energy of $H_{\mathrm{circuit}}$ only. 
% one still uses $H_{\mathrm{circuit}}$ to certify the validity of the history state.
% However, in this case, one needs to measure on the entire final state of the circuit, rather than a single output qubit,
% which can no longer be encoded solely by the ground energy.
Namely, we want to rule out the existence of any state that is far from the history state but its energy (respect to $H_{\mathrm{circuit}}$) is very close to the ground one. 
One nature approach, also ours, is to make the unique valid history state lie in the ground space of a slightly different local Hamiltonian $H'_{\mathrm{circuit}}$ that has a large \emph{spectral} gap between its ground energy and excited ones.
It is hence guaranteed that any state with a close-to-ground energy must also be close to the history state.
In other words, a certification of the energy $H'_{\mathrm{circuit}}$ could lead to a certification of the history state, which in turn helps us generate the desired sample. 
We construct such $H'_{\mathrm{circuit}}$ from $H_{\mathrm{circuit}}$ by using the \emph{perturbation} technique (e.g.,~\cite{kempe_kitaev_regev_2006}) with a further restriction to X/Z terms. (\Cref{sec:LHXZ}.)

\vspace{2mm} \noindent \emph{$\diamond$ A $\QPIP_1$ protocol for $\SampBQP$:} A certification protocol for the history state, however, is insufficient to imply a $\QPIP_1$ protocol for $\SampBQP$ directly. 
Intuitively, we face a similar dilemma as we described above about Mahadev's protocol: we can either employ the certification protocol on $H'_{\mathrm{circuit}}$ to do the test, or to measure the history state to generate the desired outcome, but not both at the same time since the testing protocol would disturb the measurement outcome.
To resolve that, we design a \emph{cut-and-choose} protocol on multiple copies of the history state so as to separate the testing and the outputting parts on separate copies of history states. 

The prover in the real protocol, of course, won't necessarily send copies of history states. Thus, to leverage the aforementioned intuition, one needs to avoid entanglement among these copies to obtain some sort of independence. 
We employ quantum \emph{de Finetti} theorem to address this technical challenge.
Specifically, given any permutation-invariant $k$-register state (where each register could contain many qubits), it is known that the reduced state on many subsets of $k$-register will be close to a separable state. 
Typically, for favorable error bounds, the parameter $k$ could be as large as the dimension of a single register, which is exponential in our context and hence undesirable. 
Fortunately, since there is no entangled operation in the protocol to perform on two copies of history states, one can employ an efficient variant of quantum \emph{de Finetti} theorem~\cite{Brandao2017} whose error bound depends poly-logarithmically on the dimension of the register, which leads to an efficient $\QPIP_1$ protocol for $\SampBQP$. 
We can further achieve very small completeness and soundness errors in this way to satisfy the premise of Mahadev's compilation.  
Note that the established $\QPIP_1$ protocol is information-theoretically sound without any computational assumption.  (\Cref{sec:qpip1}.)

\vspace{2mm} \noindent  \emph{$\diamond$ Compile $\QPIP_1$ into $\QPIP_0$ after parallel repetition}: We now discuss how to use Mahadev's measurement protocol to compile the above $\QPIP_1$ protocol for $\SampBQP$ to a $\QPIP_0$ protocol. As mentioned, a major issue we need to address in Mahadev's original construction is that when the verifier $V$ chooses to run a testing round, $V$ does not learn an output sample when it accepts.  %(which happens with probability $1/2$), 

Specifically, let $\PiNaive$ be an ``intermediate'' $\QPIP_0$ protocol obtained by applying Mahadev's compilation to the above $\QPIP_1$ protocol. Namely, when $V$ chooses to run the Hadamard round, it could learn a measurement outcome from the measurement protocol and be able to run the $\QPIP_1$ verifier to generate a decision and an output sample when accepts. However, when  $V$ chooses to run the testing round, it only decides to accept/reject without being able to output a sample. 

Similar to the above cut-and-choose idea for $\QPIP_1$, a natural idea to fix the issue is to execute multiple copies of $\PiNaive$ in parallel\footnote{It is also reasonable to consider sequential repetition, but we consider parallel repetition for its advantage of preserving the round complexity.}, and to choose a random copy to run the Hadamard round to generate an output sample and use all the remaining copies to run the testing round. The verifier accepts only when all executions accept and outputs the sample from the Hadamard round. Let us call this protocol $\PiSampZ$.

Clearly from the construction, the verifier now can output a sample when it decides to accept, and output a correct sample when interacting with an honest prover (completeness). The challenge is to show that $\PiSampZ$ is computationally sound. Note that now we are in the computational setting, we cannot use quantum de Finetti theorem as above since it only holds in the information-theoretical setting. Furthermore, parallel repetition for computationally sound protocols are typically difficult to analyze, and known to not always work for protocols with four or more messages~\cite{BIN97,PW12}.

Fortunately, parallel repetition of Mahadev's protocol for $\BQP$ has been analyzed before in ~\cite{arXiv:ChiaChungYam19, arXiv:AlaChiHun19} with interestingly different proofs. Both works showed that $m$-fold parallel repetition reduces the soundness error to $2^{-m}$ for $\BQP$ by relying on special properties of Mahadev's protocol. 

It is worth comparing parallel repetition of Mahadev's protocol for $\BQP$ and $\SampBQP$. For $\BQP$, the verifier simply chooses to run the Hadamard and testing rounds independently for each repetition. In contrast, our $\PiSampZ$ runs the Hadamard round in one repetition and run the testing round in the rest. The reason is that in $\SampBQP$, as well as generically in sampling problems, there is no known approach to combine two samples learned from two Hadamard-round executions to generate one sample with much better parameters, i.e., no generic error reduction method for the sampling problem. 
In contrast, the error reduction for decision problems can be done with the majority vote. 
As a result, while the soundness error decreases exponentially for $\BQP$, as we see below (and also in the above $\QPIP_1$ protocols), for $\SampBQP$, $m$-fold repetition only decreases the error to $\poly(1/m)$. 

We rely on the technique developed in~\cite{arXiv:ChiaChungYam19} to analyze the soundness of $\PiSampZ$. Intuitively, Mahadev's protocol has the property that if a malicious prover $P^*$ knows the answer to pass the testing round, then it must be ``committed'' to a state $\rho$ in the sense that the verifier $V$ learns the $X/Z$ measurement outcome of $\rho$ in the Hadamard round (in a computational sense). Thus, the soundness of the underlying $\QPIP_1$ protocol ensures that $V$ learns a correct sample when it accepts. In short, when $P^*$ knows the testing round answer,  $P^*$ cannot ``cheat'' in the Hadamard round. The work of~\cite{arXiv:ChiaChungYam19} formalizes this intuition by a \emph{partition lemma}, which roughly says that there is an efficient projection that partitions the prover's internal quantum space into a ``known-answer'' subspace $S_1$ where $P^*$ knows the answer to the testing round, and a ``not-known-answer'' subspace $S_0$ where $P^*$ does not know the answer to the testing round. For (efficient) prover's states $\ket{\psi} \in S_1$, the verifier $V$ can produce a sound output in the Hadamard round. For (efficient) prover's states $\ket{\psi} \in S_0$, the verifier $V$ rejects in the testing round with high probability. The work of \cite{arXiv:ChiaChungYam19} used this partition lemma iteratively to each instance of the repetition to show that for a no instance $x$ for any (efficient) prover's state $\ket{\psi}$, the probability that $V$ accepts is at most roughly $2^{-m}$.

We apply the partition lemma iteratively to the prover's state  similarly as~\cite{arXiv:ChiaChungYam19}, but with significant differences to show the soundness of $\PiSampZ$ for $\SampBQP$. In particular, extra cares are required to reason about the computational indistinguishability of the verifier's output sample from the correct distribution $D_x$. To gain some intuition, let us apply the partition lemma to the prover's state $\ket{\psi}$ with respect to the first coordinate, which decompose $\ket{\psi} = \ket{\psi_0} + \ket{\psi_1}$ with $\ket{\psi_0} \in S_0$ and  $\ket{\psi_1} \in S_1$, respectively. Intuitively, the $\ket{\psi_0}$ component will be rejected whenever the first coordinate runs the testing round, but the verifier may be ``cheated'' to accepted incorrectly when the first coordinate runs the Hadamard round, which happens with probability $1/m$ and is where the error come from. For the $\ket{\psi_1}$ component, $V$ will produce a sound output if it runs the Hadamard round at the first coordinate. For the case that $V$ chooses to run the testing round at the first coordinate, we can analyze it by further applying the partition lemma to $\ket{\psi_1}$ with respect to the second coordinate, and repeat the analysis, and so on. 
%
The intuitive analysis outlined above grosses over many technical details, and we substantiate this outline with full details in  
Section~\ref{sec:qpip0_all}.

It is worthwhile mentioning that we came to notice some online discussion\footnote{\url{https://www.scottaaronson.com/blog/?p=3697}, e.g., comment \#25, \#26, \#42, \#48. } on the possibility of a CVQC protocol for $\SampBQP$ after we developed our own result. 
These comments suggested a possible reduction of $\SampBQP$ to the local Hamiltonian problem following a similar high-level idea as our solution, however, with no detail and a seemingly different technical route. More importantly, the issue that the verifier does not generate outputs in the testing round seems to be overlooked and not discussed there.

\vspace{2mm} \noindent \textbf{A generic compiler to upgrade $\QPIP_0$ protocols with blindness}. At a high-level, the idea is simple: we run the original protocol under a QFHE with the verifier's key. Intuitively, this allows the prover to compute his next message under encryption without learning the underlying verifier's message, and hence achieves blindness while preserving the properties of the original protocol.
One subtlety with this approach is due to the fact that the verifier is classical while the QFHE cipher text could depend on both quantum and classical data.
In order to make the classical verifier work in this construction, the ciphertext and the encryption/decryption algorithm need to be classical when the underlying message is classical, which is fortunately satisfied by~\cite{mahadev_qfhe}.

A more subtle issue is to preserve the soundness.
In particular, compiled protocols with only one-time use of QFHE might (1) leak information about the circuit being evaluated during the homomorphic evaluation of QFHE ciphertexts (i.e., no \emph{circuit privacy});
or (2) fail to simulate original protocols upon receiving invalid ciphertexts.
We address these issues by letting the verifier switch to a fresh new key for each round of the protocol.
Details are given in \Cref{sec:BlindBQP2}.
%\XW{To KM: expand the above a bit more?}

\vspace{2mm} \noindent \textbf{Open Questions.} Our main focus is on the feasibility of the desired functionality and properties, which nevertheless leaves room for the improvement of parameter dependence.
Some of our parameter dependence inherits from previous works (e.g.~\cite{FOCS:Mahadev18a}), whereas some is due to our own construction where we didn't mean to identify the best possible parameter dependence. 
It will be extremely interesting to improve the parameter dependence with potentially new techniques. 

\vspace{2mm} \noindent \textbf{Organization.} We provide preliminaries on technical background in Section~\ref{sec:prelim}. 
Our simulation-based definition of CVQC for $\SampBQP$ is discussed in Section~\ref{sec:samp_definition}. 
Our main technical contributions are explained in details in Section~\ref{sec:sampbqp} (a construction of $\QPIP_1$ protocol for $\SampBQP$), 
Section~\ref{sec:qpip0_all} (the construction of $\QPIP_0$ protocol for $\SampBQP$ based on the above $\QPIP_1$ protocol), 
and Section~\ref{sec:BlindBQP2} (a generic compiler to upgrade $\QPIP_0$ protocols with blindness). 